# Complete RAG System Documentation

## ğŸ¯ System Overview

This repository contains a complete, production-ready RAG (Retrieval-Augmented Generation) system with three integrated services:

1. **ğŸ”„ LlamaIndex Ingestion Service** - Advanced document processing and ingestion
2. **ğŸ—„ï¸ Supabase Vector Database** - Hybrid search with PostgreSQL + pgvector
3. **ğŸ¤– PydanticAI Agentic RAG Agent** - Intelligent query processing with planning and reflection

## ğŸ“ Repository Structure

```
rag_system/
â”œâ”€â”€ ğŸ“ ingestion_service/           # Document ingestion pipeline
â”‚   â”œâ”€â”€ ğŸ“„ main.py                  # FastAPI service entry point
â”‚   â”œâ”€â”€ ğŸ“„ config.py                # Configuration management
â”‚   â”œâ”€â”€ ğŸ“„ requirements.txt         # Python dependencies
â”‚   â”œâ”€â”€ ğŸ“ processors/              # Document processors
â”‚   â”œâ”€â”€ ğŸ“ utils/                   # Utility functions
â”‚   â””â”€â”€ ğŸ“„ README.md               # Service documentation
â”œâ”€â”€ ğŸ“ vector_database/             # Database setup and management
â”‚   â”œâ”€â”€ ğŸ“„ setup.sql               # Main schema setup
â”‚   â”œâ”€â”€ ğŸ“„ functions.sql           # Search functions
â”‚   â”œâ”€â”€ ğŸ“ scripts/                # Deployment scripts
â”‚   â”‚   â””â”€â”€ ğŸ“„ deploy.sh           # Automated deployment
â”‚   â””â”€â”€ ğŸ“„ README.md               # Database documentation
â”œâ”€â”€ ğŸ“ agentic_rag_agent/          # Intelligent RAG agent
â”‚   â”œâ”€â”€ ğŸ“„ main.py                 # FastAPI service entry point
â”‚   â”œâ”€â”€ ğŸ“„ config.py               # Configuration management
â”‚   â”œâ”€â”€ ğŸ“„ requirements.txt        # Python dependencies
â”‚   â”œâ”€â”€ ğŸ“ agents/                 # Agent implementations
â”‚   â”œâ”€â”€ ğŸ“ models/                 # Pydantic data models
â”‚   â”œâ”€â”€ ğŸ“ tools/                  # Agent tools
â”‚   â””â”€â”€ ğŸ“„ README.md              # Agent documentation
â”œâ”€â”€ ğŸ“ docs/                       # Additional documentation
â”œâ”€â”€ ğŸ“ examples/                   # Usage examples
â”œâ”€â”€ ğŸ“ tests/                      # Test suites
â”œâ”€â”€ ğŸ“„ docker-compose.yml          # Container orchestration
â”œâ”€â”€ ğŸ“„ .env.example                # Environment template
â””â”€â”€ ğŸ“„ README.md                  # This file
```

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
# Clone the repository
git clone <repository-url>
cd rag_system

# Copy environment template
cp .env.example .env

# Edit .env with your API keys and database URLs
# Required:
# - OPENAI_API_KEY
# - SUPABASE_URL
# - SUPABASE_SERVICE_KEY
```

### 2. Database Setup

```bash
cd vector_database/

# Set environment variables
export SUPABASE_URL="your_supabase_url"
export SUPABASE_SERVICE_KEY="your_service_key"

# Deploy database schema
chmod +x scripts/deploy.sh
./scripts/deploy.sh
```

### 3. Start Services

#### Option A: Docker Compose (Recommended)
```bash
# Start all services
docker-compose up -d

# Services will be available at:
# - Ingestion Service: http://localhost:8000
# - RAG Agent: http://localhost:8001
```

#### Option B: Manual Setup
```bash
# Terminal 1: Start ingestion service
cd ingestion_service/
pip install -r requirements.txt
python main.py

# Terminal 2: Start RAG agent
cd agentic_rag_agent/
pip install -r requirements.txt
python main.py
```

### 4. Basic Usage

```python
import asyncio
import httpx

async def main():
    # 1. Ingest documents
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://localhost:8000/ingest/files",
            json={"file_paths": ["path/to/document.pdf"]}
        )
        task_id = response.json()["task_id"]
        print(f"Ingestion started: {task_id}")
    
    # 2. Query the system
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://localhost:8001/ask",
            json={
                "question": "What are the key findings in the document?",
                "enable_iteration": True,
                "enable_reflection": True
            }
        )
        result = response.json()
        print(f"Answer: {result['answer']}")
        print(f"Confidence: {result['confidence']}")

asyncio.run(main())
```

## ğŸ”§ Service Details

### Ingestion Service (Port 8000)

**Purpose**: Process and ingest documents into the vector database

**Key Features**:
- âœ… 15+ file types supported (PDF, DOCX, CSV, images, audio, code)
- âœ… Context-aware chunking with semantic boundaries
- âœ… Multi-modal processing (OCR, speech-to-text)
- âœ… Concurrent processing with configurable batch sizes
- âœ… Comprehensive metadata extraction
- âœ… Error handling and progress tracking

**API Endpoints**:
- `POST /ingest/files` - Ingest specific files
- `POST /ingest/directory` - Ingest entire directories
- `POST /ingest/upload` - Upload and ingest files
- `GET /ingest/status/{task_id}` - Check ingestion status
- `GET /health` - Service health check

### Vector Database

**Purpose**: Store and retrieve document vectors with hybrid search

**Key Features**:
- âœ… PostgreSQL + pgvector for vector similarity search
- âœ… Hybrid search combining semantic and keyword matching
- âœ… HNSW indexing for fast approximate search
- âœ… Full-text search with GIN indexes
- âœ… Advanced SQL functions for complex queries
- âœ… Performance optimizations and monitoring

**Search Functions**:
- `hybrid_search()` - Combined vector + BM25 search
- `semantic_search()` - Pure vector similarity
- `keyword_search()` - Traditional text search
- `get_document_stats()` - Database statistics

### Agentic RAG Agent (Port 8001)

**Purpose**: Intelligent question answering with planning and reflection

**Key Features**:
- âœ… Query planning and decomposition
- âœ… Iterative search with gap detection
- âœ… Self-reflection and quality assessment
- âœ… Source triangulation and verification
- âœ… Confidence scoring and transparency
- âœ… WebSocket streaming support

**API Endpoints**:
- `POST /ask` - Full agentic question answering
- `POST /ask/simple` - Basic RAG without agentic features
- `POST /ask/batch` - Batch question processing
- `GET /metrics` - Performance metrics
- `WS /ws/ask` - WebSocket streaming interface

## ğŸ—ï¸ Architecture Flow

```mermaid
graph TB
    A[Documents] --> B[Ingestion Service]
    B --> C[Document Processing]
    C --> D[Context-Aware Chunking]
    D --> E[Metadata Extraction]
    E --> F[Vector Generation]
    F --> G[Supabase Vector DB]
    
    H[User Query] --> I[RAG Agent]
    I --> J[Query Planning]
    J --> K[Iterative Search]
    K --> G
    G --> L[Hybrid Retrieval]
    L --> M[Source Triangulation]
    M --> N[Answer Generation]
    N --> O[Self-Reflection]
    O --> P[Final Response]
```

## âš™ï¸ Configuration

### Environment Variables

Create a `.env` file with:

```bash
# Required API Keys
OPENAI_API_KEY=your_openai_api_key
SUPABASE_URL=your_supabase_url
SUPABASE_SERVICE_KEY=your_supabase_service_key

# Ingestion Service
CHUNK_SIZE=1024
CHUNK_OVERLAP=200
ENABLE_OCR=true
ENABLE_SPEECH_TO_TEXT=true
MAX_CONCURRENT_FILES=5
BATCH_SIZE=100

# Vector Database
TABLE_NAME=rag_documents

# RAG Agent
LLM_MODEL=gpt-4-turbo
EMBEDDING_MODEL=text-embedding-3-large
ENABLE_RERANKING=true
MAX_ITERATIONS=3
VECTOR_WEIGHT=0.7
BM25_WEIGHT=0.3

# Performance
LOG_LEVEL=INFO
ENABLE_CACHE=true
DEBUG_MODE=false
```

### Advanced Configuration

Each service supports extensive configuration options. See individual service documentation for details:

- **Ingestion**: Chunking strategies, file type handling, OCR settings
- **Database**: Index tuning, search weights, performance optimization
- **Agent**: Agentic features, model selection, reasoning parameters

## ğŸ“Š Monitoring and Metrics

### Health Checks

```bash
# Check all services
curl http://localhost:8000/health  # Ingestion service
curl http://localhost:8001/health  # RAG agent

# Database health via agent
curl http://localhost:8001/metrics
```

### Performance Metrics

The system tracks comprehensive metrics:

- **Ingestion**: Processing time, success rates, file types
- **Search**: Query time, result quality, cache hit rates
- **Agent**: Iteration counts, confidence scores, reasoning quality

### Logging

Structured logging with configurable levels:

```python
# View logs
docker-compose logs -f ingestion_service
docker-compose logs -f rag_agent

# Configure log levels
export LOG_LEVEL=DEBUG  # DEBUG, INFO, WARNING, ERROR
```

## ğŸ§ª Testing

### Unit Tests

```bash
# Run all tests
pytest tests/

# Test specific components
pytest tests/test_ingestion.py
pytest tests/test_search.py
pytest tests/test_agent.py
```

### Integration Tests

```bash
# End-to-end testing
pytest tests/test_integration.py

# Load testing
pytest tests/test_performance.py --load-test
```

### Manual Testing

```bash
# Test ingestion
curl -X POST "http://localhost:8000/ingest/files" \
  -H "Content-Type: application/json" \
  -d '{"file_paths": ["test_document.pdf"]}'

# Test query
curl -X POST "http://localhost:8001/ask" \
  -H "Content-Type: application/json" \
  -d '{"question": "What is machine learning?"}'
```

## ğŸ“ˆ Scaling and Deployment

### Production Deployment

1. **Container Orchestration**: Use Kubernetes or Docker Swarm
2. **Load Balancing**: Deploy multiple agent instances behind a load balancer
3. **Database Scaling**: Use read replicas for search workloads
4. **Caching**: Add Redis for query and embedding caching
5. **Monitoring**: Integrate with Prometheus/Grafana

### Performance Optimization

- **Ingestion**: Increase `max_concurrent_files` and `batch_size`
- **Database**: Tune HNSW parameters and connection pooling
- **Agent**: Enable caching and adjust iteration limits

### Resource Requirements

| Component | CPU | Memory | Storage |
|-----------|-----|--------|---------|
| Ingestion Service | 2-4 cores | 4-8 GB | 10 GB |
| Vector Database | 4-8 cores | 8-32 GB | 100+ GB SSD |
| RAG Agent | 2-4 cores | 4-8 GB | 5 GB |

## ğŸ› ï¸ Development

### Adding New File Types

1. Extend `MultiModalFileProcessor` in ingestion service
2. Add type detection logic
3. Implement processing method
4. Update configuration options

### Custom Agent Tools

1. Create tool function with `@agent.tool` decorator
2. Define input/output models
3. Register with agent
4. Add to API endpoints

### Database Customization

1. Modify schema in `setup.sql`
2. Add new search functions in `functions.sql`
3. Update deployment scripts
4. Test with existing data

## ğŸš¨ Troubleshooting

### Common Issues

**1. Ingestion Failures**
```bash
# Check file permissions and size limits
ls -la /path/to/files
echo $MAX_FILE_SIZE_MB

# Review logs
docker-compose logs ingestion_service
```

**2. Search Not Working**
```bash
# Verify database connection
curl http://localhost:8001/search/test

# Check vector extension
psql -c "SELECT * FROM pg_extension WHERE extname = 'vector';"
```

**3. Performance Issues**
```bash
# Monitor resource usage
docker stats

# Check cache hit rates
curl http://localhost:8001/metrics
```

### Debug Mode

Enable detailed debugging:

```bash
export DEBUG_MODE=true
export LOG_REASONING_STEPS=true
export SAVE_INTERMEDIATE_RESULTS=true
```

## ğŸ¤ Contributing

1. Fork the repository
2.